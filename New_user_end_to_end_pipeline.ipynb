{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run process for one user inputing their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pymongo\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "import functions as fn\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import io\n",
    "from sqlalchemy import create_engine\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import string\n",
    "from emoji.unicode_codes import UNICODE_EMOJI as ue\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.cluster import cosine_distance\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather the necessary features from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 10 June 2016\n"
     ]
    }
   ],
   "source": [
    "when_did_you_open_your_account = datetime.datetime.strptime(input(\"Date when the account was opened: \"), '%d %B %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_since_account_inception = when_did_you_open_your_account - datetime.datetime.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 5\n"
     ]
    }
   ],
   "source": [
    "n_transactions_made_last_week = int(input(\"Number of transactions made last week: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number: 2\n"
     ]
    }
   ],
   "source": [
    "how_many_were_during_the_week_end = int(input(\"How many of those were during the weekend: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_made_in_week = n_transactions_made_last_week - how_many_were_during_the_week_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many of those were done yesterday: 1\n"
     ]
    }
   ],
   "source": [
    "how_many_were_yesterday = int(input(\"How many of those were done yesterday: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "n_transactions_made_to_diff_users = int(input(\"How many of those were to different users: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many days went by before you made a transaction: 2\n"
     ]
    }
   ],
   "source": [
    "max_time_between_transactions = (\n",
    "    int(input(\"What was the highest number of days that went by between two transactions: \")) * 3600 * 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How frequent where the transactions (hours) : 12\n"
     ]
    }
   ],
   "source": [
    "average_time_between_transactions = int(input(\"How frequent where the transactions (hours) : \")) * 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many of them failed : 0\n"
     ]
    }
   ],
   "source": [
    "trans_pending_of_those_made = int(input(\"How many of them failed : \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Favourite bank transfer description: Money for drugs\n"
     ]
    }
   ],
   "source": [
    "text_description = input(\"Favourite bank transfer description: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe with only the required features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_text_pattern(recomposed_note):\n",
    "    \"\"\"Function that filters through the notes, retrieves those that match\n",
    "     the specified pattern and removes stopwords.\"\"\"\n",
    "    pattern = \"([a-zA-Z0-9\\\\\\]+(?:'[a-z]+)?)\"\n",
    "    recomposed_note_raw = nltk.regexp_tokenize(recomposed_note, pattern)\n",
    "    # Create a list of stopwords and remove them from our corpus\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += list(string.punctuation)\n",
    "    # additional slang and informal versions of the original words had to be added to the corpus.\n",
    "    stopwords_list += ([\"im\", \"ur\", \"u\", \"'s\", \"n\", \"z\", \"n't\", \"brewskies\", \"mcd’s\", \"Ty$\",\n",
    "                        \"Diploooooo\", \"thx\", \"Clothessss\", \"K2\", \"B\", \"Comida\", \"yo\", \"jobby\",\n",
    "                        \"F\", \"jus\", \"bc\", \"queso\", \"fil\", \"Lol\", \"EZ\", \"RF\", \"기프트카드\", \"감사합니다\",\n",
    "                        \"Bts\", \"youuuu\", \"X’s\", \"bday\", \"WF\", \"Fooooood\", \"Yeeeeehaw\", \"temp\",\n",
    "                        \"af\", \"Chipoodle\", \"Hhuhhyhy\", \"Yummmmers\", \"MGE\", \"O\", \"Coook\", \"wahoooo\",\n",
    "                        \"Cuz\", \"y\", \"Cutz\", \"Lax\", \"LisBnB\", \"vamanos\", \"vroom\", \"Para\", \"el\", \"8==\",\n",
    "                        \"bitchhh\", \"¯\\\\_(ツ)_/¯\", \"Ily\", \"CURRYYYYYYY\", \"Depósito\", \"Yup\", \"Shhhhh\"])\n",
    "\n",
    "    recomposed_note_stopped = ([w.lower() for w in recomposed_note_raw if w not in stopwords_list])\n",
    "    return recomposed_note_stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_notes(recomposed_note_stopped):\n",
    "    \"Function that lemmatizes the different notes.\"\n",
    "    # Init Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_notes = []\n",
    "    for sentence in recomposed_note_stopped:\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            lem = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "            lemmatized_notes.append(lem)\n",
    "    return lemmatized_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_df(when_did_you_open_your_account, n_transactions_made_last_week, how_many_were_during_the_week_end,\n",
    "                how_many_were_yesterday, n_transactions_made_to_diff_users, trans_pending_of_those_made,\n",
    "                max_time_between_transactions, average_time_between_transactions, text_description):\n",
    "    \"Function that returns a specific user dataframe with the relevant inputs\"\n",
    "    user_details = {}\n",
    "    user_details['time_since_account_inception'] = (\n",
    "        datetime.datetime.today() - when_did_you_open_your_account).total_seconds()\n",
    "    user_details['n_transactions_made_last_week'] = n_transactions_made_last_week\n",
    "    user_details['n_transactions_made_during_weekend'] = how_many_were_during_the_week_end\n",
    "    user_details['n_transactions_made_during_week'] = (\n",
    "        n_transactions_made_last_week - how_many_were_during_the_week_end)\n",
    "    user_details['n_transactions_made_yesterday'] = how_many_were_yesterday\n",
    "    user_details['n_transactions_made_to_diff_users'] = n_transactions_made_to_diff_users\n",
    "    user_details['max_time_between_transactions'] = max_time_between_transactions\n",
    "    user_details['mean_time_between_transactions'] = average_time_between_transactions   \n",
    "    user_details['unsuccesful_transactions'] = trans_pending_of_those_made  \n",
    "    \n",
    "    user_details_df = pd.DataFrame([user_details])\n",
    "    \n",
    "    # Dealing with the text aspect\n",
    "    recomposed_note_stopped = get_clean_text_pattern(text_description)\n",
    "    lemmatized_notes = lemmatize_notes(recomposed_note_stopped)\n",
    "    # Load the vectorizer model\n",
    "    vectorizer= Doc2Vec.load(\"d2v.model\")\n",
    "    # Find the vectors for each note in the whole note corpus\n",
    "    _vectrs = [np.array(vectorizer.infer_vector(lemmatized_notes))]\n",
    "    _vectrs_df = pd.DataFrame(_vectrs)\n",
    "    \n",
    "    user_details_combined = pd.concat([user_details_df, _vectrs_df], axis=1)\n",
    "    return user_details_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jjherranzsarrion/anaconda3/envs/learn-env/lib/python3.6/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "user_details = get_user_df(when_did_you_open_your_account, n_transactions_made_last_week, how_many_were_during_the_week_end,\n",
    "                how_many_were_yesterday, n_transactions_made_to_diff_users, trans_pending_of_those_made,\n",
    "                max_time_between_transactions, average_time_between_transactions, text_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict transactions for a given user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "red_ent_forest_model = pickle.load(open('red_ent_forest.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_forest_y_hat_preds = red_ent_forest_model.predict_proba(user_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85855689, 0.14144311]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent_forest_y_hat_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
