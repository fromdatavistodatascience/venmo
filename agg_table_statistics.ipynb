{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull aggregated table and run first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pymongo\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "import functions as fn\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve information to connect to the database\n",
    "def get_keys(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "keys = get_keys(\"/Users/jjherranzsarrion/.secret/local_info.json\")\n",
    "username = keys['username']\n",
    "password = keys['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the venmo_transactions database\n",
    "connection = psycopg2.connect(user=f'{username}',\n",
    "                              password=f'{username}',\n",
    "                              database='venmo_transactions')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract information to calculate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features 1 and 2: Number of days the account has been opened and personalised bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"SELECT user_id, about_personalised as personalised_bio,\n",
    "       SUM(CAST('2018-07-28 23:59:59' AS timestamp) - date_joined) as time_since_account_inception\n",
    "       FROM users\n",
    "       GROUP BY (user_id, about_personalised);\"\"\"\n",
    "cursor.execute(q)\n",
    "print(\"These are the different users and the date they joined venmo\")\n",
    "user_info_df = pd.DataFrame(cursor.fetchall())\n",
    "user_info_df.columns = [x[0] for x in cursor.description]\n",
    "user_info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features 3, 4, and 5: Mean and Max time between previous transaction made and number of transactions made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"SELECT DISTINCT u.user_id, MAX(p1.diff_time) as max_time_diff_made_trans,\n",
    "              AVG(p1.diff_time) as mean_time_diff_made_trans,\n",
    "              COUNT (DISTINCT p1.payment_id) as n_transactions_made\n",
    "       FROM (SELECT p.actor_id, p.payment_id,\n",
    "                    (LEAD(p.date_created, 1) OVER (PARTITION BY p.actor_id ORDER BY p.date_created)\n",
    "                    - p.date_created) as diff_time\n",
    "             FROM payments p\n",
    "             WHERE p.date_created < CAST('2018-07-29 00:00:00' AS timestamp)) as p1\n",
    "       INNER JOIN users u ON u.user_id = p1.actor_id\n",
    "       GROUP BY (u.user_id);\"\"\"\n",
    "\n",
    "cursor.execute(q)\n",
    "payed_transactions_df = pd.DataFrame(cursor.fetchall())\n",
    "payed_transactions_df.columns = [x[0] for x in cursor.description]\n",
    "payed_transactions_df.sort_values('max_time_diff_made_trans', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features 6, 7, and 8: Mean and Max time between previous transaction received and n transactions received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"SELECT DISTINCT u.user_id, MAX(p1.diff_time) as max_time_diff_received_trans,\n",
    "              AVG(p1.diff_time) as mean_time_diff_received_trans,\n",
    "              COUNT (DISTINCT p1.payment_id) as n_transactions_received\n",
    "       FROM (SELECT p.target_user_id, p.payment_id,\n",
    "                    (LEAD(p.date_created, 1) OVER (PARTITION BY p.target_user_id ORDER BY p.date_created)\n",
    "                    - p.date_created) as diff_time\n",
    "             FROM payments p\n",
    "             WHERE p.date_created < CAST('2018-07-29 00:00:00' AS timestamp)) as p1\n",
    "       INNER JOIN users u ON u.user_id = p1.target_user_id\n",
    "       GROUP BY (u.user_id);\"\"\"\n",
    "\n",
    "cursor.execute(q)\n",
    "received_transactions_df = pd.DataFrame(cursor.fetchall())\n",
    "received_transactions_df.columns = [x[0] for x in cursor.description]\n",
    "received_transactions_df.sort_values('max_time_diff_received_trans', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 9: Total number of transactions made the previous day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q = \"\"\"SELECT u.user_id, COUNT (DISTINCT p.payment_id) as n_trans_made_yest\n",
    "       FROM payments p\n",
    "       INNER JOIN users u ON u.user_id = p.actor_id\n",
    "       WHERE p.date_created >= CAST('2018-07-28 00:00:00' AS timestamp)\n",
    "       AND p.date_created < CAST('2018-07-29 00:00:00' AS timestamp)\n",
    "       GROUP BY (u.user_id);\"\"\"\n",
    "cursor.execute(q)\n",
    "#print(\"These are the different users and the date they joined venmo\")\n",
    "trans_made_yest_df = pd.DataFrame(cursor.fetchall())\n",
    "trans_made_yest_df.columns = [x[0] for x in cursor.description]\n",
    "trans_made_yest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans_made_yest_df.sort_values('n_trans_made_yest', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange, no user made more than one transaction in the period of 2018-08-01 00:00:00 until 2018-08-07 and there were only 50 transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature 10: Total number of transactions received in the previous day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"SELECT u.user_id, COUNT (DISTINCT p.payment_id) as n_trans_rec_yest\n",
    "       FROM payments p\n",
    "       INNER JOIN users u ON u.user_id = p.target_user_id\n",
    "       WHERE p.date_created >= CAST('2018-07-28 00:00:00' AS timestamp)\n",
    "       AND p.date_created < CAST('2018-07-29 00:00:00' AS timestamp)\n",
    "       GROUP BY (u.user_id);\"\"\"\n",
    "cursor.execute(q)\n",
    "#print(\"These are the different users and the date they joined venmo\")\n",
    "trans_rec_yest_df = pd.DataFrame(cursor.fetchall())\n",
    "trans_rec_yest_df.columns = [x[0] for x in cursor.description]\n",
    "trans_rec_yest_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_rec_yest_df.sort_values('n_trans_rec_yest', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the aggregated statistics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge the received_transactions_df and the number of transactions made yesterday\n",
    "\n",
    "trans_made = pd.merge(payed_transactions_df, trans_made_yest_df, \n",
    "                      'outer', on='user_id')\n",
    "trans_made.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the total number of transactions received and number of transactions received in the past week\n",
    "\n",
    "trans_rec = pd.merge(received_transactions_df, trans_rec_yest_df, \n",
    "                     'outer', on='user_id')\n",
    "trans_rec.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the total number of user transactions, both made and received\n",
    "trans = pd.merge(trans_made, trans_rec, 'outer', on='user_id')\n",
    "trans.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the user statistics with the user information\n",
    "agg_table = pd.merge(user_info_df, trans, 'inner', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_table.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_table['time_since_account_inception'] = [diff.total_seconds() for diff in agg_table['time_since_account_inception']]\n",
    "agg_table['max_time_diff_made_trans'] = [diff.total_seconds() for diff in agg_table['max_time_diff_made_trans']]\n",
    "agg_table['max_time_diff_received_trans'] = [diff.total_seconds() for diff in agg_table['max_time_diff_received_trans']]\n",
    "agg_table['mean_time_diff_made_trans'] = [diff.total_seconds() for diff in agg_table['mean_time_diff_made_trans']]\n",
    "agg_table['mean_time_diff_received_trans'] = [diff.total_seconds() for diff in agg_table['mean_time_diff_received_trans']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_table.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting my y value\n",
    "\n",
    "q = \"\"\"SELECT u.user_id, COUNT (DISTINCT p.payment_id) as n_transactions_made_29th\n",
    "       FROM payments p\n",
    "       INNER JOIN users u ON u.user_id = p.actor_id\n",
    "       WHERE p.date_created >= CAST('2018-07-29 00:00:00' AS timestamp)\n",
    "       AND p.date_created < CAST('2018-07-30 00:00:00' AS timestamp)\n",
    "       GROUP BY (u.user_id);\"\"\"\n",
    "cursor.execute(q)\n",
    "#print(\"These are the different users and the date they joined venmo\")\n",
    "tran_or_not_df = pd.DataFrame(cursor.fetchall())\n",
    "tran_or_not_df.columns = [x[0] for x in cursor.description]\n",
    "tran_or_not_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_or_not_df['n_transactions_made_29th'] = [1 for trans in tran_or_not_df['n_transactions_made_29th']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with agg table to include nulls\n",
    "\n",
    "complete_table = pd.merge(agg_table, tran_or_not_df, 'outer', on='user_id')\n",
    "complete_table.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complete_table.drop('n_transactions_made_29th', axis=1)\n",
    "y = complete_table['n_transactions_made_29th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train_sc, y_train)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = logreg.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('\\n')\n",
    "\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('\\n')\n",
    "\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('\\n')\n",
    "\n",
    "print('Testing F1-Score: ',f1_score(y_test, y_hat_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
