{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for the independent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pymongo\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "import functions as fn\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract raw transaction details from the Mongo DB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the connection and access the venmo transactions in the MongoDB\n",
    "venmo_collection = fn.collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the initial 5% of transactions\n",
    "fn.initial_5pct(venmo_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why there is no variable assigned to the above's function result is that the function doesn't return anything. The only thing it does is generate a pickle.\n",
    "- This has to be changed when running in AWS since there will be more computer power to handle larger files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the transactions, sort and generate the tables for our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the above mentioned pickle\n",
    "with open('initial_5pct_transactions.pkl', 'rb') as f:\n",
    "    initial_5pct = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to process all transactions and extract high level transaction information\n",
    "high_level_transaction_info = fn.get_transaction_specific_information(initial_5pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to process all transactions and extract all the users details\n",
    "unique_users = fn.get_unique_user_table(initial_5pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'There are currently {len(unique_users)} unique users and {len(high_level_transaction_info)} unique transactions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to process all transactions and retrieve the different app details\n",
    "app_details = fn.get_app_specific_information(initial_5pct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the tables for our database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve information to connect to the database\n",
    "keys = fn.get_keys(\"/Users/jjherranzsarrion/.secret/local_info.json\")\n",
    "username = keys['username']\n",
    "password = keys['password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the different tables with transactions info into the venmo database\n",
    "engine = create_engine(f'postgresql://{username}:{password}@localhost/venmo_transactions')\n",
    "high_level_transaction_info.to_sql('transactions', engine)\n",
    "unique_users.to_sql('users', engine)\n",
    "app_details.to_sql('app_info', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Payments information is missing until Monday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your time ranges and add them in variables\n",
    "train_window_end = '2018-07-28 23:59:59'\n",
    "test_window_start = '2018-07-29 00:00:00'\n",
    "test_window_end = '2018-07-29 23:59:59'\n",
    "previous_day_start = '2018-07-28 00:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the aggregated statistics table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_statistics = fn.get_aggregated_user_statistics(username, password, previous_day_start, train_window_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_statistics.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_statistics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tran_or_not_df = fn.extract_target(username, password, test_window_start, test_window_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with agg table to include null values for user who did not make a transaction in the testing time period\n",
    "\n",
    "complete_table = pd.merge(user_statistics, tran_or_not_df, 'outer', on='user_id')\n",
    "complete_table.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = complete_table.drop('n_transactions_made_29th', axis=1)\n",
    "y = complete_table['n_transactions_made_29th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept = False, C = 1e12, solver='liblinear')\n",
    "model_log = logreg.fit(X_train_sc, y_train)\n",
    "model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = logreg.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('\\n')\n",
    "\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('\\n')\n",
    "\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('\\n')\n",
    "\n",
    "print('Testing F1-Score: ',f1_score(y_test, y_hat_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
