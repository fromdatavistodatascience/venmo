{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting value from the payment notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pymongo\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "import functions as fn\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from emoji.unicode_codes import UNICODE_EMOJI as ue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the above mentioned pickle\n",
    "with open('initial_5pct_transactions.pkl', 'rb') as f:\n",
    "    initial_5pct = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve information to connect to the database\n",
    "keys = fn.get_keys(\"/Users/jjherranzsarrion/.secret/local_info.json\")\n",
    "username = keys['username']\n",
    "password = keys['password']\n",
    "\n",
    "# Select your time ranges and add them in variables\n",
    "train_window_end = '2018-07-28 23:59:59'\n",
    "test_window_start = '2018-07-29 00:00:00'\n",
    "test_window_end = '2018-07-29 23:59:59'\n",
    "previous_day_start = '2018-07-28 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = fn.extracting_cursor(username, password)\n",
    "q = f\"\"\"SELECT *\n",
    "        FROM payments p\n",
    "        WHERE p.date_created <= CAST('{train_window_end}' AS timestamp);\"\"\"\n",
    "cursor.execute(q)\n",
    "payments = pd.DataFrame(cursor.fetchall())\n",
    "payments.columns = [x[0] for x in cursor.description]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = payments['note']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Food                                 2877\n",
       "ðŸ ðŸ’¸                                   2204\n",
       "ðŸ•                                    2185\n",
       "Uber                                 1951\n",
       "ðŸº                                    1355\n",
       "                                     ... \n",
       "ðŸ’™ðŸŒµ                                      1\n",
       "I just broke your glasses.. sorry       1\n",
       "ðŸ¦ðŸ¡                                      1\n",
       "ZZB                                     1\n",
       "Insurance & phone                       1\n",
       "Name: note, Length: 135985, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payments['note'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_count(notes):\n",
    "    \"\"\"Function that takes in all the notes and returns the emojis used\n",
    "    in unicode.\"\"\"\n",
    "    emoji_dict = {}\n",
    "    recomposed_note = []\n",
    "    for note in notes:\n",
    "        note_text = []\n",
    "        data = regex.findall(r'\\X', note)\n",
    "        for word in data:\n",
    "            if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "                unicode_emoji = word.encode('unicode-escape').decode('ASCII')\n",
    "                emoji_dict[word] = unicode_emoji.lower()\n",
    "                note_text.append(unicode_emoji+' ')\n",
    "            else:\n",
    "                note_text.append(word)\n",
    "        recomposed_note.append(''.join(note_text))\n",
    "    return recomposed_note, emoji_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recomposed_note, emoji_unicode = split_count(notes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ðŸ‘•': '\\\\u0001f455', 'ðŸ“±': '\\\\u0001f4f1', 'ðŸ’¸': '\\\\u0001f4b8'}"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for utilities',\n",
       " '\\\\U0001f455 !',\n",
       " 'Thank you!',\n",
       " '\\\\U0001f4f1 \\\\U0001f4b8 ',\n",
       " 'Mt Dew & candy']"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomposed_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z0-9\\\\\\]+(?:'[a-z]+)?)\"\n",
    "recomposed_note_raw = []\n",
    "for note in recomposed_note:\n",
    "    recomposed_note_raw.append(nltk.regexp_tokenize(note, pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['for', 'utilities'],\n",
       " ['\\\\U0001f455'],\n",
       " ['Thank', 'you'],\n",
       " ['\\\\U0001f4f1', '\\\\U0001f4b8'],\n",
       " ['Mt', 'Dew', 'candy']]"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomposed_note_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation)\n",
    "# additional slang and informal versions of the original words had to be added to the corpus.\n",
    "stopwords_list += ([\"im\", \"ur\", \"u\", \"'s\", \"n\", \"z\", \"n't\", \"brewskies\", \"mcdâ€™s\", \"Ty$\",\n",
    "                    \"Diploooooo\", \"thx\", \"Clothessss\", \"K2\", \"B\", \"Comida\", \"yo\", \"jobby\",\n",
    "                    \"F\", \"jus\", \"bc\", \"queso\", \"fil\", \"Lol\", \"EZ\", \"RF\", \"ê¸°í”„íŠ¸ì¹´ë“œ\", \"ê°ì‚¬í•©ë‹ˆë‹¤\",\n",
    "                    \"Bts\", \"youuuu\", \"Xâ€™s\", \"bday\", \"WF\", \"Fooooood\", \"Yeeeeehaw\", \"temp\",\n",
    "                    \"af\", \"Chipoodle\", \"Hhuhhyhy\", \"Yummmmers\", \"MGE\", \"O\", \"Coook\", \"wahoooo\",\n",
    "                    \"Cuz\", \"y\", \"Cutz\", \"Lax\", \"LisBnB\", \"vamanos\", \"vroom\", \"Para\", \"el\", \"8==\",\n",
    "                    \"bitchhh\", \"Â¯\\\\_(ãƒ„)_/Â¯\", \"Ily\", \"CURRYYYYYYY\", \"DepÃ³sito\", \"Yup\", \"Shhhhh\"])\n",
    "\n",
    "recomposed_note_stopped = []\n",
    "for note in recomposed_note_raw:\n",
    "    recomposed_note_stopped.append([w.lower() for w in note if w not in stopwords_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['utilities'],\n",
       " ['\\\\u0001f455'],\n",
       " ['thank'],\n",
       " ['\\\\u0001f4f1', '\\\\u0001f4b8'],\n",
       " ['mt', 'dew', 'candy']]"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomposed_note_stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recomposed_note_stopped_em = []\n",
    "for note in recomposed_note_stopped:\n",
    "    note_list = []\n",
    "    for word in note:\n",
    "        if word.startswith('\\\\'):\n",
    "            for key, val in emoji_unicode.items():\n",
    "                if word == val:\n",
    "                    note_list.append(key)\n",
    "        else:\n",
    "             note_list.append(word)\n",
    "    recomposed_note_stopped_em.append(note_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['utilities'], ['ðŸ‘•'], ['thank'], ['ðŸ“±', 'ðŸ’¸'], ['mt', 'dew', 'candy']]"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recomposed_note_stopped_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_count(notes_list):\n",
    "    \"\"\"Function that takes in all the notes and returns the emojis used\n",
    "    in the form of text captured by :colons:\"\"\"\n",
    "    recomposed_note = []\n",
    "    for notes in notes_list:\n",
    "        note_list = []\n",
    "        for note in notes:\n",
    "            note_text = []\n",
    "            data = regex.findall(r'\\X', note)\n",
    "            for word in data:\n",
    "                if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "                    note_text.append(emoji.demojize(f'{word}'))\n",
    "                else:\n",
    "                    note_text.append(word)\n",
    "            note_list.append(''.join(note_text))\n",
    "        recomposed_note.append(note_list)\n",
    "    return recomposed_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_recomposed_notes = split_count(recomposed_note_stopped_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['utilities'],\n",
       " [':t-shirt:'],\n",
       " ['thank'],\n",
       " [':mobile_phone:', ':money_with_wings:'],\n",
       " ['mt', 'dew', 'candy']]"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fully_recomposed_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments['recomposed_note'] = [note for note in split_count(notes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = payments['recomposed_note'][0]\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_notes = []\n",
    "for sentence in payments['recomposed_note']:\n",
    "    lemmatized_notes.append([lemmatizer.lemmatize(word, get_wordnet_pos(word)) \n",
    "                             for word in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_notes[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposed_note_freqdist = FreqDist(recomposed_note)\n",
    "recomposed_note_freqdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposed_note_stopped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments['note'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
