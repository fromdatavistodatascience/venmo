{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting value from the payment notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pymongo\n",
    "import json\n",
    "import datetime\n",
    "import pickle\n",
    "import functions as fn\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import emoji\n",
    "import regex\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import string\n",
    "from emoji.unicode_codes import UNICODE_EMOJI as ue\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info to access db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve information to connect to the database\n",
    "keys = fn.get_keys(\"/Users/jjherranzsarrion/.secret/local_info.json\")\n",
    "username = keys['username']\n",
    "password = keys['password']\n",
    "\n",
    "# Select your time ranges and add them in variables\n",
    "train_window_end = '2018-08-05 23:59:59'\n",
    "test_window_start = '2018-08-06 00:00:00'\n",
    "test_window_end = '2018-08-07 23:59:59'\n",
    "previous_day_start = '2018-08-05 00:00:00'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select all payment notes in db for the train period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_notes = fn.extract_user_notes(username, password, train_window_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = user_notes['note']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposed_note, emoji_dict = fn.get_notes_into_unicode(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposed_note_stopped = fn.get_clean_text_pattern(recomposed_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_notes = fn.lemmatize_notes(recomposed_note_stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomposed_note_stopped_em = fn.turn_emoji_unicode_to_text(lemmatized_notes, emoji_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_recomposed_notes = fn.emojis_to_text(recomposed_note_stopped_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise the text using Doc2Vec method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (time consuming)\n",
    "tagged_data = fn.train_doc2vec_vectorizer(fully_recomposed_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectrs_df = fn.get_aggregated_user_note_vector(username, password, train_window_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vectrs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using k means clustering to determine the optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the distortion plot for the different number of clusters k\n",
    "fn.get_distortion_plot(whole_corpus_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimal number of k's is 6__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LDA model to derive the topic of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and retrieve clusters (time consuming)\n",
    "lda_model = fn.get_cluster_topics_with_LDA(recomposed_note_stopped_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics of the 6 clusters are:\n",
    "- Food\n",
    "- Money_with_wings emoji\n",
    "- Pizza emoji\n",
    "- Uber\n",
    "- Clickling beer mugs emoji\n",
    "- Beer mug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer topic distributions on new, unseen documents, with\n",
    "doc_lda = lda_model[doc_bow]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
